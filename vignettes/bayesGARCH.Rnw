\documentclass[nojss,shortnames]{jss}
\usepackage{amsmath,amssymb}
\usepackage{xspace}
%% need no \usepackage{Sweave}

\author{David Ardia\\aeris CAPITAL AG, Switzerland}
\Plainauthor{David Ardia}

\title{Bayesian Estimation of the GARCH(1,1) Model:\\The \proglang{R} Package \pkg{bayesGARCH}}
\Plaintitle{Bayesian Estimation of the GARCH(1,1) Model:\\The R Package bayesGARCH} 
\Shorttitle{Bayesian Estimation of the GARCH(1,1) Model:\\The R Package bayesGARCH}

\Abstract{%
This introduction to the \proglang{R} package \pkg{bayesGARCH} is a shorter version
of \citet{Ardia:Hoogerheide:10}, published in \emph{The R Journal} and available at \url{http://journal.r-project.org/}.
The package provides functions for the Bayesian estimation of the parsimonious and
effective GARCH(1,1) model with Student-$t$ innovations.  The
estimation procedure is fully automatic and thus avoids the tedious
task of tuning an MCMC sampling algorithm.}

\Keywords{Bayesian, GARCH, MCMC, Student-$t$, \proglang{R} sofware}
\Plainkeywords{Bayesian, GARCH, MCMC, Student-t, R sofware} 

\Address{
  David Ardia\\
  aeris CAPITAL AG, Switzerland\\
  E-mail: \email{david.ardia@unifr.ch}\\
  URL: \url{http://perso.unifr.ch/david.ardia/}
}

\begin{document}

\SweaveOpts{engine=R,eps=FALSE}
%\VignetteIndexEntry{Bayesian Estimation of The GARCH Model}
%\VignetteKeywords{GARCH, Bayesian, MCMC, Student-t, R}
%\VignetteDepends{bayesGARCH,mvtnorm,coda}
%\VignettePackage{bayesGARCH}

\section{Introduction}

The package \pkg{bayesGARCH} \citep{bayesGARCH} implements the
Bayesian estimation procedure described in \citet[chapter 5]{Ardia}
for the GARCH(1,1) model with Student-$t$ innovations. The approach,
based on the work of \citet{Nakatsuma:98}, consists of a
Metropolis-Hastings (MH) algorithm where the proposal distributions
are constructed from auxiliary ARMA processes on the squared
observations. This methodology avoids the time-consuming and difficult
task, especially for non-experts, of choosing and tuning a sampling
algorithm. Moreover, in our experience, the
algorithm explores the domain of the joint posterior efficiently
compared to naive MH approaches or the Griddy-Gibbs sampler. 
The program is written in \proglang{R} with some subroutines
implemented in \proglang{C} in order to speed up the simulation procedure. The
validity of the algorithm as well as the correctness of the computer
code have been verified by the method of \citet{Geweke:04}. For further details, we 
refer the reader to \citet{Ardia:Hoogerheide:10}.

We hope that the \proglang{R} package \pkg{bayesGARCH} will be fruitful for many 
researchers like econometricians or applied statisticians. If you 
use \proglang{R} or \pkg{bayesGARCH}, please cite the software in publications.

\section{Model, priors and MCMC scheme}

A GARCH(1,1) model with Student-$t$ innovations for the log-returns
$\{ y_t \}$ may be written via data
augmentation \citep[see][]{Geweke:93} as
\begin{align}\label{e:model}
\begin{split}
y_t
&= \varepsilon_t \left(\tfrac{\nu - 2}{\nu} \, \varpi_t \, h_t\right)^{1/2} \quad t = 1,\ldots,T\\
\varepsilon_t
&\overset{\textit{iid}}{\sim} \mathcal N(0,1)\\
\varpi_t
&\overset{\textit{iid}}{\sim} \mathcal{IG} \left( \frac \nu 2, \frac \nu 2 \right)\\
h_t
&\doteq \alpha_0 + \alpha_1 y_{t-1}^2 + \beta h_{t-1}\,,
\end{split}
\end{align}
where $\alpha_0 > 0$, $\alpha_1, \beta \geq 0$ and $\nu > 2$;
$\mathcal N(0,1)$ denotes the standard normal distribution; $\mathcal
IG$ denotes the inverted gamma distribution.  The restriction on the
degrees of freedom parameter $\nu$ ensures the conditional variance to
be finite and the restrictions on the GARCH parameters
$\alpha_0, \alpha_1$ and $\beta$ guarantee its positivity.  We
emphasize the fact that only positivity constraints are implemented in
the MH algorithm; no stationarity conditions are imposed in the
simulation procedure.

In order to write the likelihood function, we define the vectors
$y \doteq (y_1,\ldots,y_T)'$, $\varpi \doteq
(\varpi_1,\ldots,\varpi_T)'$ and $\alpha \doteq
(\alpha_0, \alpha_1)'$. We regroup the model parameters into the
vector $\psi \doteq (\alpha, \beta, \nu)$. Then, upon defining the
$T \times T$ diagonal matrix
\[
\Sigma \doteq \Sigma ( \psi, \varpi ) =
\text{diag}\left( \{ \varpi_t \tfrac{\nu-2}{\nu} h_t( \alpha, \beta ) \}_{t=1}^{T} \right)\,,
\]
where $h_t(\alpha, \beta) \doteq \alpha_0 + \alpha_1 y_{t-1}^2 + \beta
h_{t-1}(\alpha, \beta)$, we can express the likelihood of
$(\psi, \varpi)$ as
\begin{align}\label{e:lik}
\mathcal L(\psi, \varpi \,|\, y)
\propto ( \det \Sigma )^{-1/2}
\exp \left[-\tfrac{1}{2} y' \Sigma^{-1} y \right]\,.
\end{align}
The Bayesian approach considers $(\psi,\varpi)$ as a random variable
which is characterized by a prior density denoted by
$p(\psi, \varpi)$. The prior is specified with the help of parameters
called hyperparameters which are initially assumed to be known and
constant. Moreover, depending on the researcher's prior information,
this density can be more or less informative. Then, by coupling the
likelihood function of the model parameters with the prior density, we
can transform the probability density using Bayes' rule to get the
posterior density $p\left(\psi, \varpi \mid y \right)$ as follows:
\begin{equation}\label{e:bayes}
p\left(\psi, \varpi \mid y \right)  = 
  \frac{\mathcal L\left(\psi, \varpi \mid y \right) p\left(\psi, \varpi \right)}
  {\int \mathcal L\left(\psi, \varpi \mid y \right) p\left(\psi, \varpi \right)    d\psi d\varpi}\,.
\end{equation}
This posterior is a quantitative, probabilistic description of the knowledge 
about the model parameters after observing the data.

We use truncated normal priors on the GARCH parameters $\alpha$ and $\beta$
\begin{align*}
p\left( \alpha \right)
&\propto \phi_{\mathcal N_2}\left( \alpha \mid \mu_\alpha, \Sigma_\alpha \right) \, \text{1} \left\{\alpha \in \mathbb R_+^2 \right\}\\
p\left( \beta \right)
&\propto \phi_{\mathcal N_1}\left( \beta \mid \mu_\beta, \Sigma_\beta \right) \, \text{1} \left\{ \beta \in \mathbb R_+ \right\} \,,
\end{align*}
where $\mu_\bullet$ and $\Sigma_\bullet$ are the hyperparameters,
$\text{1} \{\cdot\}$ is the indicator function and $\phi_{\mathcal
N_d}$ is the $d$-dimensional normal density.

The prior distribution of vector $\varpi$ conditional on $\nu$ is
found by noting that the components $\varpi_t$ are independent and
identically distributed from the inverted gamma density, which yields
\begin{align*}
p\left( \varpi \mid \nu \right)
&=
\left(
\frac{\nu}{2}
\right)^{\frac{T \nu} 2}
\left[ \Gamma \left( \frac \nu 2 \right) \right]^{-T}
\left(
\prod_{t=1}^T \varpi_t
\right)^{-\frac \nu 2 - 1} \\
&\times \exp
\left[ -\frac{1}{2} \sum_{t=1}^T \frac \nu {\varpi_t}
\right] \,.
\end{align*}
The prior distribution for the degrees-of-freedom parameter 
is a translated exponential with parameters $\lambda > 0$ and $\delta \geq 2$
\[
p\left( \nu \right) = \lambda \exp
\left[ -\lambda \left( \nu - \delta \right) \right] \, \text{1} 
\left\{\nu > \delta\right\}\,.
\]
For large values of $\lambda$, the mass of the prior is concentrated
in the neighborhood of $\delta$ and a constraint on the degrees of
freedom can be imposed in this manner. Normality of the errors is
assumed when $\delta$ is chosen large.

The joint prior distribution is then formed by assuming prior
independence between the parameters, i.e.\ $p(\psi, \varpi) =
p(\alpha)p(\beta)p(\varpi\mid\nu)p(\nu)$.

\section{Implementation}

Fitting the GARCH(1,1) model with Student-$t$ innovations to the data 
is achieved using the function \code{bayesGARCH}

\begin{CodeInput}
> args(bayesGARCH)

function (y, mu.alpha = c(0, 0),
          Sigma.alpha = 1000 * diag(1,2),
          mu.beta = 0, Sigma.beta = 1000,
          lambda = 0.01, delta = 2,
          control = list())
\end{CodeInput}

The input arguments of the function are the vector of data, the hyperparameters
and the list \code{control} which can supply any of the following elements:

\begin{itemize}
\item \code{n.chain}: number of MCMC chain(s) to be generated; default \code{1}.
\item \code{l.chain}: length of each MCMC chain; default \code{10000}.
\item \code{start.val}: vector of starting values of the chain(s); default \code{c(0.01,0.1,0.7,20)}.
\item \code{addPriorConditions}: function which allows the user to add any constraint on 
the model parameters; default \code{NULL}, i.e.\ not additional constraints are imposed.
\item \code{refresh}: frequency of reports; default \code{10}.
\item \code{digits}: number of printed digits in the reports; default \code{4}.
\end{itemize}

The function outputs the MCMC chains as an object of the
class \code{"mcmc"} from the package \pkg{coda} \citep{coda}. This
package contains functions for post-processing the MCMC output;
see \citet{Plummer:Best:Cowles:Vines:06} for an introduction. Note
that \pkg{coda} is loaded automatically with \pkg{bayesGARCH}.

\subsection{Prior restrictions and normal innovations}

The control parameter \code{addPriorConditions} can be used to impose
any type of constraints on the model parameters $\psi$ during the
estimation. For instance, to ensure the estimation of a covariance
stationary GARCH(1,1) model, the function should be defined as

\begin{CodeInput}
> addPriorConditions <- function(psi)
+   psi[2] + psi[3] < 1
\end{CodeInput}

Finally, we can impose normality of the innovations in a
straightforward manner by setting the hyperparameters $\lambda = 100$
and $\delta=500$ in the \code{bayesGARCH} function.

\subsection{Practical advice}

The estimation strategy implemented in \pkg{bayesGARCH} is fully
automatic and does not require any tuning of the MCMC sampler. This is
certainly an appealing feature for practitioners. The generation of
the Markov chains is however time consuming and estimating the model
over several datasets on a daily basis can therefore take a
significant amount of time. In this case, the algorithm can be easily
parallelized, by running a single chain on several processors.  This
can be easily achieved with the package \pkg{foreach} \citep{foreach}, for
instance. Also, when the estimation is repeated over updated time
series (i.e., time series with more recent observations), it is wise to
start the algorithm using the posterior mean or median of the
parameters obtained at the previous estimation step.  The impact of
the starting values (burn-in phase) is likely to be smaller and thus
the convergence faster.

Finally, note that as any MH algorithm, the sampler can get stuck at a
given value, so that the chain does not move anymore. However, the
sampler uses Taylor-made candidate densities that are especially
constructed at each step, so it is almost impossible for this MCMC
sampler to get stuck at a given value for many subsequent draws. In the unlikely
case that such ill behaviour does occur, one could scale the data (to
have standard deviation 1), or run the algorithm with different
initial values or a different random seed.

\section*{Disclaimer}

The views expressed in this vignette are the sole responsibility of the author and do 
not necessarily reflect those of aeris CAPITAL AG.

\begin{thebibliography}{19}
\bibitem[Ardia(2008)]{Ardia}
D. Ardia.
\newblock \emph{Financial Risk Management with Bayesian Estimation of {GARCH} Models: 
  Theory and Applications}, volume 612 of \emph{Lecture Notes in Economics and Mathematical Systems}.
\newblock Springer-Verlag, Berlin, Germany, June 2008.
\newblock ISBN 978-3-540-78656-6.
\newblock URL \url{http://www.springer.com/economics/econometrics/book/978-3-540-78656-6}.

\bibitem[Ardia(2007)]{bayesGARCH}
D. Ardia.
\newblock \emph{{bayesGARCH}: {B}ayesian Estimation of the {GARCH(1,1)} Model with {Student-t} Innovations in {R}}, 2007.
\newblock URL \url{http://CRAN.R-project.org/package=bayesGARCH}.

\bibitem[Ardia and Hoogerheide(2010)]{Ardia:Hoogerheide:10}
D. Ardia and L. F. Hoogerheide.
\newblock {B}ayesian Estimation of the {GARCH(1,1)} Model with {Student-t} Innovations.
\newblock \emph{The R Journal}, 2(2):41--47, Dec. 2010.
\newblock URL \url{http://journal.r-project.org/}

\bibitem[Geweke(2004)]{Geweke:04}
J. F. Geweke.
\newblock Getting it right: {J}oint distribution tests of posterior simulators.
\newblock \emph{Journal of the American Statistical Association}, 99(467):799--804, Sept. 2004.

\bibitem[Geweke(1993)]{Geweke:93}
J. F. Geweke.
\newblock {B}ayesian treatment of the independent {Student-t} linear model.
\newblock \emph{Journal of Applied Econometrics}, 8(S1):S19--S40, Dec. 1993.

\bibitem[Nakatsuma(1998)]{Nakatsuma:98}
T. Nakatsuma.
\newblock A {M}arkov-chain sampling algorithm for {GARCH} models.
\newblock \emph{Studies in Nonlinear Dynamics and Econometrics}, 3(2):107--117, July 1998.

\bibitem[Plummer et al.(2006)Plummer, Best, Cowles, and Vines]{Plummer:Best:Cowles:Vines:06}
M. Plummer, N. Best, K. Cowles, and K. Vines.
\newblock {CODA}: {C}onvergence diagnosis and output analysis for {MCMC}.
\newblock \emph{R News}, 6(1):7--11, Mar. 2006.

\bibitem[Plummer et al.(2010)Plummer, Best, Cowles, and Vines]{coda}
M. Plummer, N. Best, K. Cowles, and K. Vines.
\newblock \emph{{coda}: {O}utput analysis and diagnostics for {MCMC}}, 2010.
\newblock URL \url{http://CRAN.R-project.org/package=coda}.

\bibitem[REvolution Computing(2010)REvolution Computing]{foreach}
REvolution Computing.
\newblock \emph{{foreach}: Foreach looping construct for {R}}, 2009.
\newblock URL \url{http://CRAN.R-project.org/package=foreach}.

\end{thebibliography}

\end{document}
